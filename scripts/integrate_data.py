#!/usr/bin/env python3
"""
Data Integration Script for Music Recommendation System

This script processes the data generated by data_generator.py and integrates it
with the existing recommendation system components.
"""

import os
import json
import numpy as np
import pickle
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

import sys

sys.path.append(str(Path(__file__).parent.parent))

from engines.cb_engine.content_based import ContentBasedEngine
from engines.cf_engine.collaborative_filter import CollaborativeFilteringEngine
from engines.vectorizer.session_encoder import SessionVectorizer
from training.lightfm_trainer import LightFMTrainer, TrainingConfig
from api.hybrid_recommender import HybridRecommender

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DataIntegrator:
    """Integrates generated data with the recommendation system."""

    def __init__(
        self, data_dir: str = "data/generated", output_dir: str = "data/models"
    ):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # Initialize system components
        self.cb_engine = None
        self.cf_engine = None
        self.vectorizer = None
        self.hybrid = None

    def load_generated_data(self) -> Dict[str, Any]:
        """Load all generated data files."""
        logger.info("Loading generated data...")

        data = {}

        # Load tracks
        tracks_file = self.data_dir / "production_tracks.json"
        if tracks_file.exists():
            with open(tracks_file, "r") as f:
                data["tracks"] = json.load(f)
            logger.info(f"Loaded {len(data['tracks'])} tracks")

        # Load track metadata
        metadata_file = self.data_dir / "production_track_metadata.json"
        if metadata_file.exists():
            with open(metadata_file, "r") as f:
                data["metadata"] = json.load(f)
            logger.info(f"Loaded metadata for {len(data['metadata'])} tracks")

        # Load track embeddings
        embeddings_file = self.data_dir / "production_track_embeddings.json"
        if embeddings_file.exists():
            with open(embeddings_file, "r") as f:
                embeddings_data = json.load(f)
                # Convert back to numpy arrays
                data["embeddings"] = {
                    track_id: np.array(embedding)
                    for track_id, embedding in embeddings_data.items()
                }
            logger.info(f"Loaded embeddings for {len(data['embeddings'])} tracks")

        # Load sessions
        sessions_file = self.data_dir / "production_sessions.json"
        if sessions_file.exists():
            with open(sessions_file, "r") as f:
                data["sessions"] = json.load(f)
            logger.info(f"Loaded {len(data['sessions'])} user sessions")

        # Load training sessions
        training_file = self.data_dir / "production_training_sessions.json"
        if training_file.exists():
            with open(training_file, "r") as f:
                data["training_sessions"] = json.load(f)
            logger.info(f"Loaded {len(data['training_sessions'])} training sessions")

        return data

    def prepare_content_based_features(
        self, tracks: List[Dict], metadata: Dict[str, Dict]
    ) -> Dict[str, np.ndarray]:
        """Prepare content-based features from track data."""
        logger.info("Preparing content-based features...")

        features = {}

        for track in tracks:
            track_id = track["track_id"]
            track_meta = metadata.get(track_id, {})

            # Create feature vector
            feature_vector = []

            # Duration (normalized)
            duration = track_meta.get("duration", 200000)
            if duration is None:
                duration = 200000
            duration = float(duration)
            duration_norm = min(
                duration / 1000.0 / 600.0, 1.0
            )  # Convert ms to s, normalize to max 10 min
            feature_vector.append(duration_norm)

            # Year (normalized)
            year = track_meta.get("year", 2000)
            if year is None:
                year = 2000
            year = float(year)
            year_norm = (
                (year - 1950) / 70.0 if year > 1950 else 0.0
            )  # Normalize 1950-2020
            feature_vector.append(year_norm)

            # Tempo (normalized)
            tempo = track_meta.get("tempo", 120.0)
            if tempo is None:
                tempo = 120.0
            tempo = float(tempo)
            tempo_norm = tempo / 200.0  # Normalize to max 200 BPM
            feature_vector.append(tempo_norm)

            # Genre features (one-hot encoding)
            genres = track_meta.get("genres", [])
            if genres is None:
                genres = []
            common_genres = [
                "rock",
                "pop",
                "jazz",
                "electronic",
                "classical",
                "hip-hop",
                "country",
                "blues",
            ]
            for genre in common_genres:
                genre_present = (
                    1.0 if any(genre.lower() in g.lower() for g in genres) else 0.0
                )
                feature_vector.append(genre_present)

            # Mood features (one-hot encoding)
            moods = track_meta.get("moods", [])
            if moods is None:
                moods = []
            common_moods = [
                "happy",
                "sad",
                "energetic",
                "calm",
                "aggressive",
                "romantic",
            ]
            for mood in common_moods:
                mood_present = (
                    1.0 if any(mood.lower() in m.lower() for m in moods) else 0.0
                )
                feature_vector.append(mood_present)

            features[track_id] = np.array(feature_vector)

        logger.info(
            f"Created features for {len(features)} tracks with {len(feature_vector)} dimensions"
        )
        return features

    def prepare_collaborative_data(self, sessions: List[Dict]) -> tuple:
        """Prepare collaborative filtering data from sessions."""
        logger.info("Preparing collaborative filtering data...")

        # Extract unique users and items
        users = set()
        items = set()
        interactions = []

        for session in sessions:
            user_id = session["user_id"]
            users.add(user_id)

            for track_id in session["track_ids"]:
                items.add(track_id)
                interactions.append((user_id, track_id, 1.0))  # Binary interaction

        # Create mappings
        user_to_idx = {user: idx for idx, user in enumerate(sorted(users))}
        item_to_idx = {item: idx for idx, item in enumerate(sorted(items))}

        # Create interaction matrix
        num_users = len(users)
        num_items = len(items)

        logger.info(
            f"Created CF data: {num_users} users, {num_items} items, {len(interactions)} interactions"
        )

        return {
            "users": list(users),
            "items": list(items),
            "interactions": interactions,
            "user_to_idx": user_to_idx,
            "item_to_idx": item_to_idx,
            "num_users": num_users,
            "num_items": num_items,
        }

    def train_content_based_engine(
        self, features: Dict[str, np.ndarray], embeddings: Dict[str, np.ndarray]
    ):
        """Train and save content-based engine."""
        logger.info("Training content-based engine...")

        self.cb_engine = ContentBasedEngine()

        # Save features
        features_file = self.output_dir / "cb_features.pkl"
        with open(features_file, "wb") as f:
            pickle.dump(features, f)

        # Save embeddings if available
        if embeddings:
            embeddings_file = self.output_dir / "cb_embeddings.pkl"
            with open(embeddings_file, "wb") as f:
                pickle.dump(embeddings, f)

        logger.info("Content-based engine data saved")

    def train_collaborative_engine(self, cf_data: Dict):
        """Train and save collaborative filtering engine."""
        logger.info("Training collaborative filtering engine...")

        # Initialize LightFM trainer with config
        trainer = LightFMTrainer(
            config=TrainingConfig(
                no_components=64, loss="warp", learning_rate=0.05, epochs=10
            )
        )

        # Build dataset and interaction matrix
        interactions = [(str(u), str(i), r) for (u, i, r) in cf_data["interactions"]]
        trainer.build_dataset(interactions)
        interactions_matrix, weights = trainer.build_interaction_matrix(interactions)

        # Train the model
        trainer.train(interactions_matrix)

        # Save the model and mappings
        model_path = self.output_dir / "lightfm_model.pkl"
        mappings_path = self.output_dir / "lightfm_mappings.pkl"
        trainer.save_model(model_path)
        # Optionally, save embeddings or mappings if needed

        # Initialize CF engine
        self.cf_engine = CollaborativeFilteringEngine(
            model_path=str(model_path), mappings_path=str(mappings_path)
        )

        logger.info("Collaborative filtering engine trained and saved")

    def train_session_encoders(self, sessions: List[Dict]):
        """Train and save session encoders."""
        logger.info("Training session encoders...")

        # Initialize vectorizer
        self.vectorizer = SessionVectorizer()

        # Prepare session data for training
        session_sequences = []
        for session in sessions:
            if len(session["track_ids"]) >= 3:  # Minimum session length
                session_sequences.append(session["track_ids"])

        # Save session data for encoder training
        sessions_file = self.output_dir / "training_sessions.pkl"
        with open(sessions_file, "wb") as f:
            pickle.dump(session_sequences, f)

        logger.info(
            f"Session data saved for training: {len(session_sequences)} sessions"
        )

    def create_hybrid_system(self):
        """Create and save hybrid recommendation system."""
        logger.info("Creating hybrid recommendation system...")

        # Initialize hybrid system
        self.hybrid = HybridRecommender(cb_weight=0.6, cf_weight=0.4)

        # Configuration
        config = {
            "cb_features_path": str(self.output_dir / "cb_features.pkl"),
            "cb_embeddings_path": str(self.output_dir / "cb_embeddings.pkl"),
            "cf_model_path": str(self.output_dir / "lightfm_model.pkl"),
            "cf_mappings_path": str(self.output_dir / "lightfm_mappings.pkl"),
            "sessions_path": str(self.output_dir / "training_sessions.pkl"),
        }

        config_file = self.output_dir / "system_config.json"
        with open(config_file, "w") as f:
            json.dump(config, f, indent=2)

        logger.info("Hybrid system configuration saved")

    def integrate_all(self):
        """Main integration pipeline."""
        logger.info("Starting data integration pipeline...")

        # Load generated data
        data = self.load_generated_data()

        if not data:
            logger.error("No data found to integrate!")
            return

        # Prepare content-based features
        if "tracks" in data and "metadata" in data:
            cb_features = self.prepare_content_based_features(
                data["tracks"], data["metadata"]
            )
            embeddings = data["embeddings"]  # Use the loaded embeddings
            self.train_content_based_engine(cb_features, embeddings)
            # --- Save features as .npy for API compatibility ---
            import pickle

            track_ids = list(sorted(cb_features.keys()))
            with open(self.output_dir.parent / "metadata_features.npy", "wb") as f:
                pickle.dump(cb_features, f)
            # Optionally, save track_ids for reference
            import json

            with open(
                self.output_dir.parent / "metadata_features_track_ids.json", "w"
            ) as f:
                json.dump(track_ids, f, indent=2)

        # Prepare collaborative filtering data
        cf_files_exist = False
        if "sessions" in data and data["sessions"]:
            cf_data = self.prepare_collaborative_data(data["sessions"])
            self.train_collaborative_engine(cf_data)
            cf_files_exist = True
        else:
            logger.warning(
                "No sessions found: skipping collaborative filtering model generation."
            )

        # Train session encoders
        if "training_sessions" in data and data["training_sessions"]:
            self.train_session_encoders(data["training_sessions"])
        elif "sessions" in data and data["sessions"]:
            self.train_session_encoders(data["sessions"])

        # Create hybrid system config (only reference files that exist)
        config = {
            "cb_features_path": str(self.output_dir / "cb_features.pkl"),
            "cb_embeddings_path": str(self.output_dir / "cb_embeddings.pkl"),
            "sessions_path": str(self.output_dir / "training_sessions.pkl"),
        }
        if cf_files_exist:
            config["cf_model_path"] = str(self.output_dir / "lightfm_model.pkl")
            config["cf_mappings_path"] = str(self.output_dir / "lightfm_mappings.pkl")
        config_file = self.output_dir / "system_config.json"
        with open(config_file, "w") as f:
            json.dump(config, f, indent=2)
        logger.info("Hybrid system configuration saved")
        logger.info("Data integration completed successfully!")
        logger.info(f"Model files saved to: {self.output_dir}")


def main():
    """Main function for command line usage."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Integrate generated data with recommendation system"
    )
    parser.add_argument(
        "--data-dir",
        default="data/generated",
        help="Directory containing generated data files",
    )
    parser.add_argument(
        "--output-dir",
        default="data/models",
        help="Directory to save trained models and features",
    )

    args = parser.parse_args()

    integrator = DataIntegrator(args.data_dir, args.output_dir)
    integrator.integrate_all()


if __name__ == "__main__":
    main()
